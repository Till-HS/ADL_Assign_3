{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc086b5-dde2-49ad-bcc4-738c7db35d26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T20:25:41.659844Z",
     "iopub.status.busy": "2025-04-15T20:25:41.659106Z",
     "iopub.status.idle": "2025-04-15T20:25:48.957149Z",
     "shell.execute_reply": "2025-04-15T20:25:48.956493Z",
     "shell.execute_reply.started": "2025-04-15T20:25:41.659797Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-15 22:25:46.859251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744748746.879544  241746 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744748746.885685  241746 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744748746.901631  241746 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744748746.901654  241746 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744748746.901656  241746 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744748746.901658  241746 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-15 22:25:46.906836: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "import evaluate\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed5d98ca-b72d-47ad-acc6-2ef8563595d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T14:52:07.315113Z",
     "iopub.status.busy": "2025-04-15T14:52:07.314271Z",
     "iopub.status.idle": "2025-04-15T14:52:07.327592Z",
     "shell.execute_reply": "2025-04-15T14:52:07.326210Z",
     "shell.execute_reply.started": "2025-04-15T14:52:07.315071Z"
    }
   },
   "outputs": [],
   "source": [
    "# the given loading function, modified for BERT:\n",
    "def preprocess_pandas(data, columns):\n",
    "    df_ = pd.DataFrame(columns=columns)\n",
    "    data['Sentence'] = data['Sentence'].str.lower()\n",
    "    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
    "    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
    "    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]','')                                                       # remove special characters\n",
    "    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                   # remove numbers\n",
    "    for index, row in data.iterrows():\n",
    "        word_tokens = word_tokenize(row['Sentence'])\n",
    "        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
    "        df_.loc[len(df_)] = {\n",
    "            \"index\": row['index'],\n",
    "            \"Class\": row['Class'],\n",
    "            \"Sentence\": \" \".join(filtered_sent)\n",
    "        }\n",
    "    return data\n",
    "\n",
    "def load_data(data_path):\n",
    "    # get data, pre-process and split\n",
    "    data = pd.read_csv(data_path, delimiter='\\t', header=None)\n",
    "    data.columns = ['Sentence', 'Class']\n",
    "    data['index'] = data.index                                          # add new column index\n",
    "    columns = ['index', 'Class', 'Sentence']\n",
    "    data = preprocess_pandas(data, columns)                             # pre-process\n",
    "    training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "        data['Sentence'].values.astype('U'),\n",
    "        data['Class'].values.astype('int32'),\n",
    "        test_size=0.10,\n",
    "        random_state=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return training_data, validation_data, training_labels, validation_labels\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4a72982-82e1-4321-b3f7-34a68080848f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T00:55:54.550017Z",
     "iopub.status.busy": "2025-04-15T00:55:54.549824Z",
     "iopub.status.idle": "2025-04-15T00:55:56.140428Z",
     "shell.execute_reply": "2025-04-15T00:55:56.139415Z",
     "shell.execute_reply.started": "2025-04-15T00:55:54.550001Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "sequence_classification_model = torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', 'bert-base-cased-finetuned-mrpc')\n",
    "sequence_classification_tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased-finetuned-mrpc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d30f5dc-d1a7-40c2-af20-7ea15f7ff752",
   "metadata": {},
   "source": [
    "# Evaluation of pretrained, not finetuned model\n",
    "\n",
    "First, a pretrained \"Bert for sequence classification\" model trained on performing binary sentiment analysis on yelp reviews is performed. The creators claim an accuracy of 0.9699 on the original yelp dataset (https://huggingface.co/textattack/bert-base-uncased-yelp-polarity).\n",
    "\n",
    "Our hypothesis is that this can be transferred to amazon reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f9be2d-bd30-46b0-8277-14e1400a9dd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T00:55:56.141704Z",
     "iopub.status.busy": "2025-04-15T00:55:56.141419Z",
     "iopub.status.idle": "2025-04-15T00:56:09.189143Z",
     "shell.execute_reply": "2025-04-15T00:56:09.188100Z",
     "shell.execute_reply.started": "2025-04-15T00:55:56.141680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "0.92\n",
      "Validation data:\n",
      "0.93\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
    "\n",
    "training_data, validation_data, training_labels, validation_labels = load_data(\"amazon_cells_labelled.txt\")\n",
    "\n",
    "#### For SMALL dataset TRAINING data\n",
    "encoded_inputs = tokenizer(list(training_data), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_ids_training = logits.argmax(dim=1).tolist()\n",
    "# evaluation\n",
    "print(\"Training data:\")\n",
    "print(sum(training_labels == predicted_class_ids_training)/len(training_labels))\n",
    "\n",
    "### For SMALL dataset VALIDATION data\n",
    "\n",
    "encoded_inputs = tokenizer(list(validation_data), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_ids_validation = logits.argmax(dim=1).tolist()\n",
    "# evaluation\n",
    "print(\"Validation data:\")\n",
    "print(sum(validation_labels == predicted_class_ids_validation)/len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1f9981-8f02-4f01-b81f-d60949815c23",
   "metadata": {},
   "source": [
    "As can be seen above, the pretrained but not finetuned model already performs surprisingy well with correctly identifying 92 % of the sentiments in the smaller (1000) dataset's training dataset and 93 % of the validation dataset. Some sentences that where misclassified can be found in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "572b3c97-0a20-4203-8c40-e630dbaf8543",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T00:56:09.191006Z",
     "iopub.status.busy": "2025-04-15T00:56:09.190585Z",
     "iopub.status.idle": "2025-04-15T00:56:09.201002Z",
     "shell.execute_reply": "2025-04-15T00:56:09.200405Z",
     "shell.execute_reply.started": "2025-04-15T00:56:09.190983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT\t Pred.\t Sequence\n",
      "0 \t 1 \t the one big drawback of the mp player is that the buttons on the phone's front cover that let you pause and skip songs lock out after a few seconds.\n",
      "1 \t 0 \t nice sound.\n",
      "1 \t 0 \t it plays louder than any other speaker of this size; the price is so low that most would think the quality is lacking, however, it's not.\n",
      "1 \t 0 \t no shifting, no bubbling, no peeling, not even a scratch, nothing!i couldn't be more happier with my new one for the droid.\n",
      "1 \t 0 \t also its slim enough to fit into my alarm clock docking station without removing the case.\n",
      "0 \t 1 \t when i placed my treo into the case, not only was it not snug, but there was a lot of extra room on the sides.\n",
      "0 \t 1 \t battery lasts only a few hours.\n",
      "1 \t 0 \t gets a signal when other verizon phones won't.\n",
      "0 \t 1 \t this product is very high quality chinese crap!!!!!!\n",
      "0 \t 1 \t lasted one day and then blew up.\n",
      "0 \t 1 \t returned  hours later.\n",
      "0 \t 1 \t it's so stupid to have to keep buying new chargers, car chargers, cradles, headphones and car kits every time a new phone comes out.\n",
      "1 \t 0 \t i ended up sliding it on the edge of my pants or back pockets instead.\n",
      "1 \t 0 \t the noise shield is incrediable.\n",
      "1 \t 0 \t it definitely was not as good as my s.\n",
      "0 \t 1 \t the only thing that i think could improve is the sound leaks out from the headset.\n",
      "1 \t 0 \t those phones are working just fine now.\n",
      "0 \t 1 \t not loud enough and doesn't turn on like it should.\n",
      "0 \t 1 \t the only thing that disappoint me is the infra red port (irda).\n",
      "0 \t 1 \t i'd like to return it.\n",
      "0 \t 1 \t even in my bmw  series which is fairly quiet, i have trouble hearing what the other person is saying.\n",
      "1 \t 0 \t the eargels channel the sound directly into your ear and seem to increase the sound volume and clarity.\n",
      "0 \t 1 \t the phone gets extremely hot!\n",
      "1 \t 0 \t great phone.\n",
      "1 \t 0 \t the \".\" mega pixel camera, being a part of a phone, is reasonably good.\n",
      "1 \t 0 \t phone now holds charge like it did when it was new.\n",
      "1 \t 0 \t in the span of an hour, i had two people exclaim \"whoa - is that the new phone on tv?!?\n",
      "1 \t 0 \t i was very excited to get this headset because i thought it was really cute.\n",
      "1 \t 0 \t i can hear while i'm driving in the car, and usually don't even have to put it on it's loudest setting.\n",
      "1 \t 0 \t great charger.\n",
      "0 \t 1 \t excellent starter wireless headset.\n",
      "1 \t 0 \t great phone.\n",
      "0 \t 1 \t the replacement died in a few weeks.\n",
      "0 \t 1 \t i might have gotten a defect, but i would not risk buying it again because of the built quality alone.\n",
      "0 \t 1 \t so there is no way for me to plug it in here in the us unless i go by a converter.\n",
      "1 \t 0 \t the range is very decent, i've been able to roam around my house with the phone in the living room with no reception/sound quality issues.\n",
      "1 \t 0 \t battery charge-life is quite long.\n",
      "0 \t 1 \t this is the first phone i've had that has been so cheaply made.\n",
      "0 \t 1 \t the loudspeaker option is great, the bumpers with the lights is very ... appealing.\n",
      "0 \t 1 \t the plastic breaks really easy on this clip.\n",
      "1 \t 0 \t if you are razr owner...you must have this!\n",
      "0 \t 1 \t then i had to continue pairing it periodically since it somehow kept dropping.\n",
      "0 \t 1 \t does not charge the cingular (att)  phone.\n",
      "0 \t 1 \t there's really nothing bad i can say about this headset.\n",
      "0 \t 1 \t the screen does get smudged easily because it touches your ear and face.\n",
      "1 \t 0 \t no ear loop needed, it's tiny and the sound is great.\n",
      "0 \t 1 \t all i can do is whine on the internet, so here it goes.the more i use the thing the less i like it.\n",
      "1 \t 0 \t motorola finally got the voice quality of a bluetooth headset right.\n",
      "0 \t 1 \t makes it easier to keep up with my bluetooth when i'm not wearing it.\n",
      "1 \t 0 \t protects the phone on all sides.\n",
      "1 \t 0 \t great phone.\n",
      "1 \t 0 \t my sanyo has survived dozens of drops on blacktop without ill effect.\n",
      "1 \t 0 \t much less than the jawbone i was going to replace it with.\n",
      "1 \t 0 \t this case has passed the one year mark and while it shows signs of wear, it is % functional.\n",
      "1 \t 0 \t this phone is slim and light and the display is beautiful.\n",
      "1 \t 0 \t its a total package.\n",
      "1 \t 0 \t linked to my phone without effort.\n",
      "0 \t 1 \t i'll be looking for a new earpiece.\n",
      "1 \t 0 \t works great, when my cat attacked the phone he scratched the protective strip instead of destroying the screen.\n",
      "1 \t 0 \t i usually don't like headbands but this one is very lightweight & doesn't mess up my hair.\n",
      "1 \t 0 \t it is so small and you don't even realize that it is there after a while of getting used to it.\n",
      "1 \t 0 \t i have yet to run this new battery below two bars and that's three days without charging.\n",
      "0 \t 1 \t i'm returning them.\n",
      "0 \t 1 \t one thing i hate is the mode set button at the side.\n",
      "1 \t 0 \t great phone.\n",
      "1 \t 0 \t the only good thing was that it fits comfortably on small ears.\n",
      "0 \t 1 \t however, my girl was complain that some time the phone doesn't wake up like normal phone does.\n",
      "1 \t 0 \t it finds my cell phone right away when i enter the car.\n",
      "1 \t 0 \t appears to actually outperform the original battery from china that came with my vi.\n",
      "0 \t 1 \t i bought two of them and neither will charge.\n",
      "0 \t 1 \t think it over when you plan to own this one!this sure is the last moto phone for me!\n",
      "0 \t 1 \t chinese forgeries abound!.\n"
     ]
    }
   ],
   "source": [
    "print(\"GT\\t Pred.\\t Sequence\")\n",
    "\n",
    "for i in range(len(training_labels)):\n",
    "    if training_labels[i] != predicted_class_ids_training[i]:\n",
    "        print(training_labels[i], \"\\t\", predicted_class_ids_training[i], \"\\t\", training_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb828f-3c9b-4928-bd33-57cb011ff7f7",
   "metadata": {},
   "source": [
    "For most of the sequences it is obvious why there is a misclassification, such as a short text length; and formulations as \"the only thing that disappoints me is...\" which is labelled as \"negative sentiment\" in the GT but implies that the whole review is mostly positive, which is picked up by Bert. Also, the term \"China\" seems to be associated with negative sentiments which is not true for sentences as \"appears to actually outperform the original battery from china that came with my vi.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0580da90-c137-4951-944f-55b45b6b9c31",
   "metadata": {},
   "source": [
    "# Finetuning the model\n",
    "on the small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d0773d-9750-4642-b934-d9ce7cfb787c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T00:56:09.201884Z",
     "iopub.status.busy": "2025-04-15T00:56:09.201720Z",
     "iopub.status.idle": "2025-04-15T00:58:13.140744Z",
     "shell.execute_reply": "2025-04-15T00:58:13.139549Z",
     "shell.execute_reply.started": "2025-04-15T00:56:09.201870Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 900/900 [00:00<00:00, 3308.81 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 3600.48 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [339/339 01:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.080682</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.113173</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.119065</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=339, training_loss=0.11766228914964164, metrics={'train_runtime': 118.6846, 'train_samples_per_second': 22.749, 'train_steps_per_second': 2.856, 'total_flos': 710399849472000.0, 'train_loss': 0.11766228914964164, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data, validation_data, training_labels, validation_labels = load_data(\"amazon_cells_labelled.txt\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\", num_labels=2)\n",
    "\n",
    "#### For SMALL dataset TRAINING data\n",
    "#encoded_training = tokenizer(list(training_data), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "training_dataset = Dataset.from_dict({\"text\": training_data, \"label\": training_labels})\n",
    "training_dataset = training_dataset.map(tokenize, batched=True)\n",
    "\n",
    "#encoded_validation = tokenizer(list(validation_data), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "validation_dataset = Dataset.from_dict({\"text\": validation_data, \"label\": validation_labels})\n",
    "validation_dataset = validation_dataset.map(tokenize, batched=True)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # convert the logits to their predicted class\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"amazon_review_classifier\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False, \n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24aa82b9-7f9e-41fd-8388-efaa3cd551c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T00:58:13.142223Z",
     "iopub.status.busy": "2025-04-15T00:58:13.141925Z",
     "iopub.status.idle": "2025-04-15T00:58:13.251244Z",
     "shell.execute_reply": "2025-04-15T00:58:13.250293Z",
     "shell.execute_reply.started": "2025-04-15T00:58:13.142188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data:\n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "### For SMALL dataset VALIDATION data\n",
    "\n",
    "encoded_inputs = tokenizer(list(validation_data), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "encoded_inputs = encoded_inputs.to(\"cuda\")\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_ids_validation = logits.argmax(dim=1).tolist()\n",
    "# evaluation\n",
    "print(\"Validation data:\")\n",
    "print(sum(validation_labels == predicted_class_ids_validation)/len(validation_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf774c5-304e-442b-87b5-8e5b59effa03",
   "metadata": {},
   "source": [
    "As can be seen, even by finetuning on the small dataset, the accuracy on the validation data can be increased to 96%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50d98f15-cbc4-4c08-81ab-0165fdb8e79f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T00:58:13.252659Z",
     "iopub.status.busy": "2025-04-15T00:58:13.252383Z",
     "iopub.status.idle": "2025-04-15T00:58:13.258840Z",
     "shell.execute_reply": "2025-04-15T00:58:13.257789Z",
     "shell.execute_reply.started": "2025-04-15T00:58:13.252633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t 0 \t you'll love how thin it is.\n",
      "0 \t 1 \t battery life still not long enough in motorola razor vi.\n",
      "0 \t 1 \t the real killer is the volume, and of course it breaking.\n",
      "0 \t 1 \t the biggest complaint i have is, the battery drains superfast.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(validation_labels)):\n",
    "    if validation_labels[i] != predicted_class_ids_validation[i]:\n",
    "        print(validation_labels[i], \"\\t\", predicted_class_ids_validation[i], \"\\t\", validation_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa6de87-7aae-4ba6-880a-aaf740fe677f",
   "metadata": {},
   "source": [
    "The accuracy is really good on the validation dataset, one of the few sequences that are labeled wrong is \"the real killer is the volume, and of course it breaking.\", which is seen as negative in the GT but as positive by the classifier. It is probably very hard for a language model to detect sarcasm, even more without context.\n",
    "\n",
    "But the validation dataset only contains 100 sequences, so probably we were just lucky. Let's test the model on the validation part of the big dataset (25000 lines)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "830b4067-ccc7-4a34-b238-c5b9556a0aca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T00:58:13.260234Z",
     "iopub.status.busy": "2025-04-15T00:58:13.259871Z",
     "iopub.status.idle": "2025-04-15T00:59:23.880105Z",
     "shell.execute_reply": "2025-04-15T00:59:23.879258Z",
     "shell.execute_reply.started": "2025-04-15T00:58:13.260195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data:\n",
      "0.922\n"
     ]
    }
   ],
   "source": [
    "### For LARGE dataset VALIDATION data\n",
    "training_data, validation_data, training_labels, validation_labels = load_data(\"amazon_cells_labelled_LARGE_25K.txt\")\n",
    "\n",
    "encoded_inputs = tokenizer(list(validation_data), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "encoded_inputs = encoded_inputs.to(\"cuda\")\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_ids_validation = logits.argmax(dim=1).tolist()\n",
    "# evaluation\n",
    "print(\"Validation data:\")\n",
    "print(sum(validation_labels == predicted_class_ids_validation)/len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5026e-3bdd-44a4-a104-a03a111f0e89",
   "metadata": {},
   "source": [
    "This does not look much better than the pretrained model. So let's train it on the whole training data of the 25k dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25806f60-d2ad-4e71-9c46-713ae67d50b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T01:20:59.580989Z",
     "iopub.status.busy": "2025-04-15T01:20:59.580765Z",
     "iopub.status.idle": "2025-04-15T02:03:58.192713Z",
     "shell.execute_reply": "2025-04-15T02:03:58.191949Z",
     "shell.execute_reply.started": "2025-04-15T01:20:59.580975Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 22500/22500 [00:06<00:00, 3537.89 examples/s]\n",
      "Map: 100%|██████████| 2500/2500 [00:00<00:00, 3579.86 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8439' max='8439' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8439/8439 42:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.321800</td>\n",
       "      <td>0.241744</td>\n",
       "      <td>0.930400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.175900</td>\n",
       "      <td>0.249191</td>\n",
       "      <td>0.936400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.142400</td>\n",
       "      <td>0.297812</td>\n",
       "      <td>0.934800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8439, training_loss=0.20060898769182933, metrics={'train_runtime': 2568.5533, 'train_samples_per_second': 26.279, 'train_steps_per_second': 3.286, 'total_flos': 1.77599962368e+16, 'train_loss': 0.20060898769182933, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\", num_labels=2)\n",
    "\n",
    "#### For SMALL dataset TRAINING data\n",
    "#encoded_training = tokenizer(list(training_data), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "training_dataset = Dataset.from_dict({\"text\": training_data, \"label\": training_labels})\n",
    "training_dataset = training_dataset.map(tokenize, batched=True)\n",
    "\n",
    "#encoded_validation = tokenizer(list(validation_data), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "validation_dataset = Dataset.from_dict({\"text\": validation_data, \"label\": validation_labels})\n",
    "validation_dataset = validation_dataset.map(tokenize, batched=True)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # convert the logits to their predicted class\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"amazon_review_classifier\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False, \n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cd05384-5390-4140-8801-92073b5d74b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T02:04:43.359646Z",
     "iopub.status.busy": "2025-04-15T02:04:43.359405Z",
     "iopub.status.idle": "2025-04-15T02:05:54.671642Z",
     "shell.execute_reply": "2025-04-15T02:05:54.670810Z",
     "shell.execute_reply.started": "2025-04-15T02:04:43.359630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data:\n",
      "0.9304\n"
     ]
    }
   ],
   "source": [
    "### For LARGE dataset VALIDATION data\n",
    "training_data, validation_data, training_labels, validation_labels = load_data(\"amazon_cells_labelled_LARGE_25K.txt\")\n",
    "\n",
    "encoded_inputs = tokenizer(list(validation_data), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "encoded_inputs = encoded_inputs.to(\"cuda\")\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_ids_validation = logits.argmax(dim=1).tolist()\n",
    "# evaluation\n",
    "print(\"Validation data:\")\n",
    "print(sum(validation_labels == predicted_class_ids_validation)/len(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235990ba-3894-4206-a0fe-4d17e5ce6063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T20:26:02.041897Z",
     "iopub.status.busy": "2025-04-15T20:26:02.041269Z",
     "iopub.status.idle": "2025-04-15T20:26:02.054228Z",
     "shell.execute_reply": "2025-04-15T20:26:02.052550Z",
     "shell.execute_reply.started": "2025-04-15T20:26:02.041874Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_pandas(data, columns):\n",
    "    df_ = pd.DataFrame(columns=columns)\n",
    "    data['Sentence'] = data['Sentence'].str.lower()\n",
    "    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
    "    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
    "    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]','')                                                       # remove special characters\n",
    "    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                   # remove numbers\n",
    "    for index, row in data.iterrows():\n",
    "        word_tokens = word_tokenize(row['Sentence'])\n",
    "        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
    "        df_.loc[len(df_)] = {\n",
    "            \"index\": row['index'],\n",
    "            \"Class\": row['Class'],\n",
    "            \"Sentence\": \" \".join(filtered_sent)\n",
    "        }\n",
    "    return data\n",
    "\n",
    "# If this is the primary file that is executed (ie not an import of another file)\n",
    "def load_data_EXTRALARGE(data_path, max_lines = None, val_size = 0.1):\n",
    "    if max_lines == None:\n",
    "        training_xlarge = pd.read_csv(data_path, delimiter=',', header=None)        \n",
    "    else:\n",
    "        training_xlarge = pd.read_csv(data_path, delimiter=',', header=None).sample(max_lines, random_state = 0)            # choose a sample of size 200000\n",
    "    #test_xlarge = pd.read_csv(\"test.csv\", delimiter=',', header=None)\n",
    "    training_xlarge.columns = [ 'Class', 'Title', 'Sentence']\n",
    "    training_xlarge['index'] = training_xlarge.index                                          # add new column index\n",
    "    columns = ['index', 'Class', 'Sentence']\n",
    "    training_xlarge = preprocess_pandas(training_xlarge, columns)                             # pre-process\n",
    "    test_size = val_size\n",
    "    training_data_xl, validation_data_xl, training_labels_xl, validation_labels_xl = train_test_split( # split the data into training, validation, and test splits\n",
    "        training_xlarge['Sentence'].values.astype('U'),\n",
    "        training_xlarge['Class'].values.astype('int32'),\n",
    "        test_size=0.10,\n",
    "        random_state=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    training_labels_xl -= 1                       # mapping from {1, 2} to {0, 1}\n",
    "    validation_labels_xl -= 1\n",
    "    return training_data_xl, validation_data_xl, training_labels_xl, validation_labels_xl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f0e5eec-88a3-48c3-a05f-74c819f3d11e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T02:24:20.944024Z",
     "iopub.status.busy": "2025-04-15T02:24:20.943627Z",
     "iopub.status.idle": "2025-04-15T08:49:09.167075Z",
     "shell.execute_reply": "2025-04-15T08:49:09.165611Z",
     "shell.execute_reply.started": "2025-04-15T02:24:20.944006Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 180000/180000 [01:04<00:00, 2785.85 examples/s]\n",
      "Map: 100%|██████████| 20000/20000 [00:06<00:00, 2909.50 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67500' max='67500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67500/67500 5:47:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.302400</td>\n",
       "      <td>0.280662</td>\n",
       "      <td>0.918450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.236600</td>\n",
       "      <td>0.303261</td>\n",
       "      <td>0.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.174800</td>\n",
       "      <td>0.288119</td>\n",
       "      <td>0.934100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=67500, training_loss=0.26074552216706454, metrics={'train_runtime': 20868.6396, 'train_samples_per_second': 25.876, 'train_steps_per_second': 3.235, 'total_flos': 1.420799698944e+17, 'train_loss': 0.26074552216706454, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data, validation_data, training_labels, validation_labels = load_data_EXTRALARGE(\"test.csv\", 200000)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\", num_labels=2)\n",
    "\n",
    "#### For SMALL dataset TRAINING data\n",
    "#encoded_training = tokenizer(list(training_data), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "training_dataset = Dataset.from_dict({\"text\": training_data, \"label\": training_labels})\n",
    "training_dataset = training_dataset.map(tokenize, batched=True)\n",
    "\n",
    "#encoded_validation = tokenizer(list(validation_data), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "validation_dataset = Dataset.from_dict({\"text\": validation_data, \"label\": validation_labels})\n",
    "validation_dataset = validation_dataset.map(tokenize, batched=True)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # convert the logits to their predicted class\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"amazon_review_classifier\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66702c21-29e2-40b9-9d0c-b8be40d87ca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T09:33:50.361232Z",
     "iopub.status.busy": "2025-04-15T09:33:50.360658Z",
     "iopub.status.idle": "2025-04-15T09:33:51.118020Z",
     "shell.execute_reply": "2025-04-15T09:33:51.117305Z",
     "shell.execute_reply.started": "2025-04-15T09:33:50.361189Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model_trained_on_EXTRALARGE_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb0cb588-5efe-4827-9b48-1b07e412a973",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:07:41.350020Z",
     "iopub.status.busy": "2025-04-15T10:07:41.349647Z",
     "iopub.status.idle": "2025-04-15T10:09:08.697281Z",
     "shell.execute_reply": "2025-04-15T10:09:08.696392Z",
     "shell.execute_reply.started": "2025-04-15T10:07:41.349994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the sentiments in the test dataset:\n",
      "positive:  0.5234765234765235 \t negative:  0.4765234765234765\n",
      "Accuracy:\n",
      "0.957042957042957\n",
      "Precision: \t 0.9656488549618321\n",
      "Recall: \t 0.9529190207156308\n",
      "F1 Score: \t 0.9592417061611375\n",
      "with regard to positive sentiments.\n",
      "Prediction runtime: \t 87.33383631706238\n"
     ]
    }
   ],
   "source": [
    "### For LARGE dataset VALIDATION data\n",
    "import time\n",
    "start = time.time()\n",
    "training_data, validation_data, training_labels, validation_labels = load_data_EXTRALARGE(\"test.csv\", max_lines = 10001, val_size = 10000)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"model_trained_on_EXTRALARGE_dataset\").to(\"cuda\")\n",
    "encoded_inputs = tokenizer(list(validation_data), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "encoded_inputs = encoded_inputs.to(\"cuda\")\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_ids_validation = logits.argmax(dim=1).tolist()\n",
    "# evaluation\n",
    "end = time.time()\n",
    "print(\"Distribution of the sentiments in the test dataset:\")\n",
    "print(\"positive: \", sum(validation_labels == 1)/len(validation_labels), \"\\t negative: \", 1-sum(validation_labels == 1)/len(validation_labels))\n",
    "print(\"Accuracy:\")\n",
    "print(sum(validation_labels == predicted_class_ids_validation)/len(validation_labels))\n",
    "TP = sum( (validation_labels == predicted_class_ids_validation) & (validation_labels == 1) )\n",
    "FP = sum( (validation_labels != predicted_class_ids_validation) & (validation_labels == 1) )\n",
    "FN = sum( (validation_labels != predicted_class_ids_validation) & (validation_labels == 0) )\n",
    "F1 = 2*TP / (2*TP + FP + FN)\n",
    "\n",
    "print(\"Precision: \\t\", TP/(TP+FP))\n",
    "print(\"Recall: \\t\", TP/(TP+FN))\n",
    "print(\"F1 Score: \\t\", F1)\n",
    "\n",
    "print(\"with regard to positive sentiments.\")\n",
    "print(\"Prediction runtime: \\t\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb77bcb-2dbc-4775-ae6b-9b9e74b58f01",
   "metadata": {},
   "source": [
    "# But is it really necessary to use an already finetuned model?\n",
    "\n",
    "Let's try a blank BERT model, and to save time, we will only train on 100,000 lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7e9c0fb-61c1-4f12-b990-4265dfb1f217",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:46:31.420143Z",
     "iopub.status.busy": "2025-04-15T16:46:31.419525Z",
     "iopub.status.idle": "2025-04-15T17:00:45.850664Z",
     "shell.execute_reply": "2025-04-15T17:00:45.849330Z",
     "shell.execute_reply.started": "2025-04-15T16:46:31.420106Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data, validation_data, training_labels, validation_labels = load_data_EXTRALARGE(\"test.csv\", 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f33d67c-8d07-4aa1-8859-f2c5ab01bc37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:00:45.852743Z",
     "iopub.status.busy": "2025-04-15T17:00:45.852524Z",
     "iopub.status.idle": "2025-04-15T20:09:08.634691Z",
     "shell.execute_reply": "2025-04-15T20:09:08.633692Z",
     "shell.execute_reply.started": "2025-04-15T17:00:45.852722Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 90000/90000 [00:33<00:00, 2722.33 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:03<00:00, 2828.63 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33750' max='33750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33750/33750 3:07:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.257800</td>\n",
       "      <td>0.261475</td>\n",
       "      <td>0.928700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.181700</td>\n",
       "      <td>0.369631</td>\n",
       "      <td>0.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.107100</td>\n",
       "      <td>0.299778</td>\n",
       "      <td>0.938400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    \n",
    "#### For SMALL dataset TRAINING data\n",
    "#encoded_training = tokenizer(list(training_data), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "training_dataset = Dataset.from_dict({\"text\": training_data, \"label\": training_labels})\n",
    "training_dataset = training_dataset.map(tokenize, batched=True)\n",
    "\n",
    "#encoded_validation = tokenizer(list(validation_data), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "validation_dataset = Dataset.from_dict({\"text\": validation_data, \"label\": validation_labels})\n",
    "validation_dataset = validation_dataset.map(tokenize, batched=True)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # convert the logits to their predicted class\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"amazon_review_classifier\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "model.save_pretrained(\"model_trained_on_EXTRALARGE_dataset_not_pretrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb040668-f00b-4e7d-aaef-8c514af00d9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T20:26:10.408352Z",
     "iopub.status.busy": "2025-04-15T20:26:10.407624Z",
     "iopub.status.idle": "2025-04-15T20:27:29.731616Z",
     "shell.execute_reply": "2025-04-15T20:27:29.730449Z",
     "shell.execute_reply.started": "2025-04-15T20:26:10.408306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the sentiments in the test dataset:\n",
      "positive:  0.5234765234765235 \t negative:  0.4765234765234765\n",
      "Accuracy:\n",
      "0.987012987012987\n",
      "Precision: \t 0.9923664122137404\n",
      "Recall: \t 0.9829867674858223\n",
      "F1 Score: \t 0.9876543209876543\n",
      "with regard to positive sentiments.\n",
      "Prediction runtime: \t 79.30403590202332\n"
     ]
    }
   ],
   "source": [
    "### For LARGE dataset VALIDATION data\n",
    "import time\n",
    "start = time.time()\n",
    "training_data, validation_data, training_labels, validation_labels = load_data_EXTRALARGE(\"test.csv\", max_lines = 10001, val_size = 10000)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"model_trained_on_EXTRALARGE_dataset_not_pretrained\").to(\"cuda\")\n",
    "encoded_inputs = tokenizer(list(validation_data), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "encoded_inputs = encoded_inputs.to(\"cuda\")\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_ids_validation = logits.argmax(dim=1).tolist()\n",
    "# evaluation\n",
    "end = time.time()\n",
    "print(\"Distribution of the sentiments in the test dataset:\")\n",
    "print(\"positive: \", sum(validation_labels == 1)/len(validation_labels), \"\\t negative: \", 1-sum(validation_labels == 1)/len(validation_labels))\n",
    "print(\"Accuracy:\")\n",
    "print(sum(validation_labels == predicted_class_ids_validation)/len(validation_labels))\n",
    "TP = sum( (validation_labels == predicted_class_ids_validation) & (validation_labels == 1) )\n",
    "FP = sum( (validation_labels != predicted_class_ids_validation) & (validation_labels == 1) )\n",
    "FN = sum( (validation_labels != predicted_class_ids_validation) & (validation_labels == 0) )\n",
    "F1 = 2*TP / (2*TP + FP + FN)\n",
    "\n",
    "print(\"Precision: \\t\", TP/(TP+FP))\n",
    "print(\"Recall: \\t\", TP/(TP+FN))\n",
    "print(\"F1 Score: \\t\", F1)\n",
    "\n",
    "print(\"with regard to positive sentiments.\")\n",
    "print(\"Prediction runtime: \\t\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0285c7-3375-45f2-a831-e25171dc5c0a",
   "metadata": {},
   "source": [
    "Turns out that this is **even better**, an accuracy of 99.2 % is reached compared to the 95.7 % of the model that was finetuned on yelp reviews before. It shows that it can be a good idea to check whether a blank top layer performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21195376",
   "metadata": {},
   "source": [
    "# Task 3: Comparison:\n",
    "As can be seen above, tranformer models offer a much higher precision in Language Processing than normal feed forward neural networks. They runtime of the models is also mostly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd248cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
