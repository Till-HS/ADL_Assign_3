{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T11:10:50.386394Z",
     "start_time": "2025-04-08T11:10:45.873341Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T13:38:10.861156Z",
     "iopub.status.busy": "2025-04-15T13:38:10.860272Z",
     "iopub.status.idle": "2025-04-15T13:38:16.130152Z",
     "shell.execute_reply": "2025-04-15T13:38:16.129496Z",
     "shell.execute_reply.started": "2025-04-15T13:38:10.861108Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 15:38:14.035426: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744724294.055092  234895 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744724294.062005  234895 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744724294.078575  234895 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744724294.078601  234895 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744724294.078604  234895 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744724294.078606  234895 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-15 15:38:14.085793: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1218ec8091ef33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T11:10:51.756311Z",
     "start_time": "2025-04-08T11:10:51.046734Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T13:38:16.131083Z",
     "iopub.status.busy": "2025-04-15T13:38:16.130757Z",
     "iopub.status.idle": "2025-04-15T13:38:16.134519Z",
     "shell.execute_reply": "2025-04-15T13:38:16.133724Z",
     "shell.execute_reply.started": "2025-04-15T13:38:16.131066Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 50000\n",
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c80e1d9bb2a0131",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T11:10:54.762456Z",
     "start_time": "2025-04-08T11:10:53.707641Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T13:38:16.136122Z",
     "iopub.status.busy": "2025-04-15T13:38:16.135920Z",
     "iopub.status.idle": "2025-04-15T13:38:16.145677Z",
     "shell.execute_reply": "2025-04-15T13:38:16.144974Z",
     "shell.execute_reply.started": "2025-04-15T13:38:16.136106Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_pandas(data, columns):\n",
    "    df_ = pd.DataFrame(columns=columns)\n",
    "    data['Sentence'] = data['Sentence'].str.lower()\n",
    "    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
    "    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
    "    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]','')                                                       # remove special characters\n",
    "    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                   # remove numbers\n",
    "    for index, row in data.iterrows():\n",
    "        word_tokens = word_tokenize(row['Sentence'])\n",
    "        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
    "        df_.loc[len(df_)] = {\n",
    "            \"index\": row['index'],\n",
    "            \"Class\": row['Class'],\n",
    "            \"Sentence\": \" \".join(filtered_sent)\n",
    "        }\n",
    "    return data\n",
    "\n",
    "def load_and_process_data(data_path):\n",
    "    # get data, pre-process and split\n",
    "    data = pd.read_csv(data_path, delimiter='\\t', header=None)\n",
    "    data.columns = ['Sentence', 'Class']\n",
    "    data['index'] = data.index                                          # add new column index\n",
    "    columns = ['index', 'Class', 'Sentence']\n",
    "    data = preprocess_pandas(data, columns)                             # pre-process\n",
    "    training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "        data['Sentence'].values.astype('U'),\n",
    "        data['Class'].values.astype('int32'),\n",
    "        test_size=0.10,\n",
    "        random_state=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "    word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=MAX_FEATURES, max_df=0.5, use_idf=True, norm='l2')\n",
    "    training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "    training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
    "    vocab_size = len(word_vectorizer.vocabulary_)\n",
    "    validation_data = word_vectorizer.transform(validation_data)\n",
    "    validation_data = validation_data.todense()\n",
    "    train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "    train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "    validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "    validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()\n",
    "\n",
    "    return train_x_tensor,train_y_tensor,validation_x_tensor,validation_y_tensor,word_vectorizer\n",
    "\n",
    "def pad(twoDTensor):\n",
    "    res = torch.zeros(twoDTensor.size(0),MAX_FEATURES).type(torch.FloatTensor)\n",
    "    res[:,:twoDTensor.size(1)] = twoDTensor\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "585876aa5ce713b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T11:10:55.189238Z",
     "start_time": "2025-04-08T11:10:54.984834Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T13:38:16.146315Z",
     "iopub.status.busy": "2025-04-15T13:38:16.146156Z",
     "iopub.status.idle": "2025-04-15T13:38:16.154707Z",
     "shell.execute_reply": "2025-04-15T13:38:16.154174Z",
     "shell.execute_reply.started": "2025-04-15T13:38:16.146300Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__() #input of size\n",
    "        self.fc1 = nn.Linear(size, (int)(size/8))\n",
    "        #self.fc2 = nn.Linear((int)(size/2), (int)(size/4)) # to size/4\n",
    "        #self.fc3 = nn.Linear((int)(size/4), (int)(size/8))\n",
    "        self.fc4 = nn.Linear((int)(size/8), (int)(size/16)) # to size/4^2\n",
    "        #self.fc5 = nn.Linear((int)(size/16), (int)(size/32))\n",
    "        self.fc6 = nn.Linear((int)(size/16), (int)(size/64)) # to size/4^3\n",
    "        #self.fc7 = nn.Linear((int)(size/64), (int)(size/128))\n",
    "        self.fc8 = nn.Linear((int)(size/64), (int)(size/256)) # to size/4^4\n",
    "        self.fc9 = nn.Linear((int)(size/256), 2) # output of 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        #x = F.leaky_relu(self.fc2(x))\n",
    "        #x = F.leaky_relu(self.fc3(x))\n",
    "        x = F.leaky_relu(self.fc4(x))\n",
    "        #x = F.leaky_relu(self.fc5(x))\n",
    "        x = F.leaky_relu(self.fc6(x))\n",
    "        #x = F.leaky_relu(self.fc7(x))\n",
    "        x = F.leaky_relu(self.fc8(x))\n",
    "        x = self.fc9(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3de6800a2bcc0fc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T11:10:56.377284Z",
     "start_time": "2025-04-08T11:10:56.192944Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T13:38:16.155714Z",
     "iopub.status.busy": "2025-04-15T13:38:16.155557Z",
     "iopub.status.idle": "2025-04-15T13:38:16.167797Z",
     "shell.execute_reply": "2025-04-15T13:38:16.167121Z",
     "shell.execute_reply.started": "2025-04-15T13:38:16.155700Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, writer, BATCH_SIZE):\n",
    "    # Initialization\n",
    "    early_stopping = 0\n",
    "    epochs_done = 0\n",
    "    min_LSTM_loss = float('inf') #10000\n",
    "\n",
    "    # Training and validation\n",
    "    for i in range(num_epochs):\n",
    "        epochs_done += 1\n",
    "        epoch_training_loss = 0\n",
    "        epoch_validation_loss = 0\n",
    "\n",
    "        # Evaluation of the model\n",
    "        correct = 0\n",
    "        model.eval()\n",
    "        for _,(inputs,labels) in enumerate(val_loader):\n",
    "            # One-hot encode labels\n",
    "            labels_one_hot = F.one_hot(labels,2)\n",
    "\n",
    "            # Convert images and labels to tensor\n",
    "            inputs = torch.Tensor(inputs).type('torch.FloatTensor')\n",
    "            labels_one_hot = torch.Tensor(labels_one_hot).type('torch.FloatTensor')\n",
    "\n",
    "            # Make sure everything is done on GPU\n",
    "            inputs = inputs.to(device)\n",
    "            labels_one_hot = labels_one_hot.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model.forward(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels_one_hot)\n",
    "\n",
    "            # Update loss\n",
    "            epoch_validation_loss += loss.cpu().detach().numpy()\n",
    "\n",
    "            #compute accuracy\n",
    "            preds = torch.argmax(outputs,1)\n",
    "            correct += (preds==labels).float().sum()\n",
    "        v_acc = correct/(len(val_loader)*BATCH_SIZE)\n",
    "        writer.add_scalar('Accuracy/evaluation' ,v_acc ,i)\n",
    "        v_loss = epoch_validation_loss/len(val_loader.dataset)\n",
    "        writer.add_scalar('Loss/evaluation', v_loss, i)\n",
    "\n",
    "        if loss<min_LSTM_loss:\n",
    "            torch.save(model, 'best_model.pt')\n",
    "            min_LSTM_loss = loss\n",
    "            early_stopping = 0\n",
    "        else:\n",
    "            early_stopping += 1\n",
    "\n",
    "        if early_stopping >= 15:\n",
    "            break\n",
    "\n",
    "        # Train model\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        for _,(inputs,labels) in enumerate(train_loader):\n",
    "            # One-hot encode labels\n",
    "            labels_one_hot = F.one_hot(labels,2)\n",
    "\n",
    "            # Convert images and labels to tensor\n",
    "            inputs = torch.Tensor(inputs).type('torch.FloatTensor')\n",
    "            labels_one_hot = torch.Tensor(labels_one_hot).type('torch.FloatTensor')\n",
    "\n",
    "            # Make sure everything is done on GPU\n",
    "            inputs = inputs.to(device)\n",
    "            labels_one_hot = labels_one_hot.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model.forward(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels_one_hot)\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update loss\n",
    "            epoch_training_loss += loss.cpu().detach().numpy()\n",
    "\n",
    "            #compute accuracy\n",
    "            preds = torch.argmax(outputs,1)\n",
    "            correct += (preds==labels).float().sum()\n",
    "        t_acc = correct/(len(train_loader)*BATCH_SIZE)\n",
    "        writer.add_scalar('Accuracy/training' ,t_acc ,i)\n",
    "        t_loss = epoch_training_loss/len(train_loader.dataset)\n",
    "        writer.add_scalar('Loss/training', t_loss, i)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch {i}, training loss: {t_loss}, training accuracy: {t_acc}, validation loss: {v_loss}, validation accuracy: {v_acc}')\n",
    "\n",
    "    best_model = torch.load('best_model.pt', weights_only=False)\n",
    "    return best_model,epochs_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aeab4c457d13dd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T11:16:30.216324Z",
     "start_time": "2025-04-08T11:10:56.493720Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T13:17:44.744115Z",
     "iopub.status.busy": "2025-04-15T13:17:44.743550Z",
     "iopub.status.idle": "2025-04-15T13:17:46.657978Z",
     "shell.execute_reply": "2025-04-15T13:17:46.657268Z",
     "shell.execute_reply.started": "2025-04-15T13:17:44.744081Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading the first data\n",
    "train_x,train_y,val_x,val_y,vectorizer = load_and_process_data(\"amazon_cells_labelled.txt\")\n",
    "#train_x = pad(train_x)\n",
    "#val_x = pad(val_x)\n",
    "size = train_x.size(1)\n",
    "\n",
    "\n",
    "trainset = TensorDataset(train_x,train_y)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "valset = TensorDataset(val_x, val_y)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8406f274b088f039",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T11:16:31.060486300Z",
     "start_time": "2025-04-08T07:52:39.066258Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T13:17:46.659465Z",
     "iopub.status.busy": "2025-04-15T13:17:46.658996Z",
     "iopub.status.idle": "2025-04-15T13:17:46.804032Z",
     "shell.execute_reply": "2025-04-15T13:17:46.803404Z",
     "shell.execute_reply.started": "2025-04-15T13:17:46.659442Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining the model\n",
    "net = Net(size)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 200\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c55ba26fb8e883eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T11:16:31.293328400Z",
     "start_time": "2025-04-08T07:52:39.302835Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T13:02:38.456717Z",
     "iopub.status.busy": "2025-04-15T13:02:38.456325Z",
     "iopub.status.idle": "2025-04-15T13:03:03.006916Z",
     "shell.execute_reply": "2025-04-15T13:03:03.005945Z",
     "shell.execute_reply.started": "2025-04-15T13:02:38.456698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 0.03220616653561592, training accuracy: 0.495555579662323, validation loss: 0.014222116209566593, validation accuracy: 0.4699999988079071\n",
      "Epoch 10, training loss: 2.24794050041055e-08, training accuracy: 1.0, validation loss: 0.024489451199769974, validation accuracy: 0.8199999928474426\n",
      "19 epoch reached\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=7277, out_features=3638, bias=True)\n",
       "  (fc2): Linear(in_features=3638, out_features=1819, bias=True)\n",
       "  (fc3): Linear(in_features=1819, out_features=909, bias=True)\n",
       "  (fc4): Linear(in_features=909, out_features=454, bias=True)\n",
       "  (fc5): Linear(in_features=454, out_features=227, bias=True)\n",
       "  (fc6): Linear(in_features=227, out_features=113, bias=True)\n",
       "  (fc7): Linear(in_features=113, out_features=56, bias=True)\n",
       "  (fc8): Linear(in_features=56, out_features=28, bias=True)\n",
       "  (fc9): Linear(in_features=28, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer = SummaryWriter(comment='fully_connected-small_db')\n",
    "best_model, epoch_reached = train_model(net, criterion, optimizer, trainloader, valloader, epochs, writer, BATCH_SIZE)\n",
    "print(f'{epoch_reached} epoch reached')\n",
    "writer.close()\n",
    "net.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a83b87cb-5f2f-44fa-8072-210eb5e15681",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:38:16.168427Z",
     "iopub.status.busy": "2025-04-15T13:38:16.168275Z",
     "iopub.status.idle": "2025-04-15T13:39:35.709066Z",
     "shell.execute_reply": "2025-04-15T13:39:35.708009Z",
     "shell.execute_reply.started": "2025-04-15T13:38:16.168413Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading the next data\n",
    "train_x,train_y,val_x,val_y,vectorizer2 = load_and_process_data(\"amazon_cells_labelled_LARGE_25K.txt\")\n",
    "#train_x = pad(train_x)\n",
    "#val_x = pad(val_x)\n",
    "size2 = train_x.size(1)\n",
    "\n",
    "\n",
    "trainset = TensorDataset(train_x,train_y)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "valset = TensorDataset(val_x, val_y)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c1a991a-48cd-4dbb-ae0b-f4d8d4bcac45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:39:35.711119Z",
     "iopub.status.busy": "2025-04-15T13:39:35.710923Z",
     "iopub.status.idle": "2025-04-15T13:39:40.663408Z",
     "shell.execute_reply": "2025-04-15T13:39:40.662673Z",
     "shell.execute_reply.started": "2025-04-15T13:39:35.711102Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining the model\n",
    "net = Net(size2)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 200\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87440c8d-e2bd-4ec8-a03f-3afd96fc88e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:39:40.664646Z",
     "iopub.status.busy": "2025-04-15T13:39:40.664027Z",
     "iopub.status.idle": "2025-04-15T13:49:00.022988Z",
     "shell.execute_reply": "2025-04-15T13:49:00.022199Z",
     "shell.execute_reply.started": "2025-04-15T13:39:40.664628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 0.01013773400336504, training accuracy: 0.8224889039993286, validation loss: 0.01391210313886404, validation accuracy: 0.4043999910354614\n",
      "Epoch 10, training loss: 0.19886238873004913, training accuracy: 0.9863555431365967, validation loss: 7.835793972015381, validation accuracy: 0.8611999750137329\n",
      "17 epoch reached\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(comment='fully_connected-large_db')\n",
    "best_model2, epoch_reached2 = train_model(net, criterion, optimizer, trainloader, valloader, epochs, writer, BATCH_SIZE)\n",
    "print(f'{epoch_reached2} epoch reached')\n",
    "writer.close()\n",
    "net.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f1288b4-1302-4505-b511-8e7610b3522f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:49:00.025053Z",
     "iopub.status.busy": "2025-04-15T13:49:00.024767Z",
     "iopub.status.idle": "2025-04-15T13:49:00.032245Z",
     "shell.execute_reply": "2025-04-15T13:49:00.031633Z",
     "shell.execute_reply.started": "2025-04-15T13:49:00.025031Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_EXTRALARGE(data_path, word_vectorizer, max_lines = None, val_size = 0.1):\n",
    "    if max_lines == None:\n",
    "        training_xlarge = pd.read_csv(data_path, delimiter=',', header=None)        \n",
    "    else:\n",
    "        training_xlarge = pd.read_csv(data_path, delimiter=',', header=None).sample(max_lines, random_state = 0)            # choose a sample of size 200000\n",
    "    #test_xlarge = pd.read_csv(\"test.csv\", delimiter=',', header=None)\n",
    "    training_xlarge.columns = [ 'Class', 'Title', 'Sentence']\n",
    "    training_xlarge['index'] = training_xlarge.index                                          # add new column index\n",
    "    columns = ['index', 'Class', 'Sentence']\n",
    "    training_xlarge = preprocess_pandas(training_xlarge, columns)                             # pre-process\n",
    "    test_size = val_size\n",
    "    training_data_xl, validation_data_xl, training_labels_xl, validation_labels_xl = train_test_split( # split the data into training, validation, and test splits\n",
    "        training_xlarge['Sentence'].values.astype('U'),\n",
    "        training_xlarge['Class'].values.astype('int32'),\n",
    "        test_size=0.10,\n",
    "        random_state=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    training_labels_xl -= 1                       # mapping from {1, 2} to {0, 1}\n",
    "    validation_labels_xl -= 1\n",
    "\n",
    "    # vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "    training_data_xl = word_vectorizer.transform(training_data_xl)        # transform texts to sparse matrix\n",
    "    training_data_xl = training_data_xl.todense()                             # convert to dense matrix for Pytorch\n",
    "    validation_data_xl = word_vectorizer.transform(validation_data_xl)\n",
    "    validation_data_xl = validation_data_xl.todense()\n",
    "    training_data_xl = torch.from_numpy(np.array(training_data_xl)).type(torch.FloatTensor)\n",
    "    training_labels_xl = torch.from_numpy(np.array(training_labels_xl)).long()\n",
    "    validation_data_xl = torch.from_numpy(np.array(validation_data_xl)).type(torch.FloatTensor)\n",
    "    validation_labels_xl = torch.from_numpy(np.array(validation_labels_xl)).long()\n",
    "    \n",
    "    return training_data_xl, validation_data_xl, training_labels_xl, validation_labels_xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87bd58c4-269c-40bd-9a67-75d253e02eb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:49:00.033151Z",
     "iopub.status.busy": "2025-04-15T13:49:00.032883Z",
     "iopub.status.idle": "2025-04-15T13:49:00.045102Z",
     "shell.execute_reply": "2025-04-15T13:49:00.044591Z",
     "shell.execute_reply.started": "2025-04-15T13:49:00.033134Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def test(network, word_vectorizer):\n",
    "    start = time.time()\n",
    "    training_data, validation_data, training_labels, validation_labels = load_data_EXTRALARGE(\"test.csv\", word_vectorizer, max_lines = 10001, val_size = 10000)\n",
    "    #validation_data = pad(validation_data)\n",
    "    \n",
    "    ### your prediction\n",
    "    #here goes the result:\n",
    "    network.to(\"cpu\")\n",
    "    outputs = network.forward(validation_data)\n",
    "    predicted_class_ids_validation = torch.argmax(outputs,1)\n",
    "    # evaluation\n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Distribution of the sentiments in the test dataset:\")\n",
    "    print(\"positive: \", sum(validation_labels == 1)/len(validation_labels), \"\\t negative: \", 1-sum(validation_labels == 1)/len(validation_labels))\n",
    "    print(\"Accuracy:\")\n",
    "    print(sum(validation_labels == predicted_class_ids_validation)/len(validation_labels))\n",
    "    TP = sum( (validation_labels == predicted_class_ids_validation) & (validation_labels == 1) )\n",
    "    FP = sum( (validation_labels != predicted_class_ids_validation) & (validation_labels == 1) )\n",
    "    FN = sum( (validation_labels != predicted_class_ids_validation) & (validation_labels == 0) )\n",
    "    F1 = 2*TP / (2*TP + FP + FN)\n",
    "    \n",
    "    print(\"Precision: \\t\", TP/(TP+FP))\n",
    "    print(\"Recall: \\t\", TP/(TP+FN))\n",
    "    print(\"F1 Score: \\t\", F1)\n",
    "    \n",
    "    print(\"with regard to positive sentiments.\")\n",
    "    print(\"Prediction runtime: \\t\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0301cbc7-c557-48b5-a200-77aaac661a2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:03:39.083602Z",
     "iopub.status.busy": "2025-04-15T13:03:39.083321Z",
     "iopub.status.idle": "2025-04-15T13:04:52.965685Z",
     "shell.execute_reply": "2025-04-15T13:04:52.964917Z",
     "shell.execute_reply.started": "2025-04-15T13:03:39.083581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the sentiments in the test dataset:\n",
      "positive:  tensor(0.5235) \t negative:  tensor(0.4765)\n",
      "Accuracy:\n",
      "tensor(0.5005)\n",
      "Precision: \t tensor(0.0534)\n",
      "Recall: \t tensor(0.8750)\n",
      "F1 Score: \t tensor(0.1007)\n",
      "with regard to positive sentiments.\n",
      "Prediction runtime: \t 73.82173180580139\n"
     ]
    }
   ],
   "source": [
    "test(best_model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aaccdcb-6a38-4fca-9163-eac3aa8850d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:49:00.045836Z",
     "iopub.status.busy": "2025-04-15T13:49:00.045630Z",
     "iopub.status.idle": "2025-04-15T13:50:19.666787Z",
     "shell.execute_reply": "2025-04-15T13:50:19.666022Z",
     "shell.execute_reply.started": "2025-04-15T13:49:00.045821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the sentiments in the test dataset:\n",
      "positive:  tensor(0.5235) \t negative:  tensor(0.4765)\n",
      "Accuracy:\n",
      "tensor(0.8571)\n",
      "Precision: \t tensor(0.8817)\n",
      "Recall: \t tensor(0.8508)\n",
      "F1 Score: \t tensor(0.8660)\n",
      "with regard to positive sentiments.\n",
      "Prediction runtime: \t 79.41915130615234\n"
     ]
    }
   ],
   "source": [
    "test(best_model2, vectorizer2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
